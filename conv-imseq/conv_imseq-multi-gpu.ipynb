{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup:\n",
    "\n",
    "Select GPUs before starting this ipython server. As an example, to use /gpu:4 and /gpu:6 on the server, use the following command:\n",
    "\n",
    "*$ export CUDA_VISIBLE_DEVICES=4,6*\n",
    "\n",
    "In case this variable is not set or mulitple devices are selected, TensorFlow will allocate memory on **all** devices, but will run only on /gpu:0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Force matplotlib to use inline rendering\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add path to libraries for ipython\n",
    "sys.path.append(os.path.expanduser(\"~/libs\"))\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensortools as tt\n",
    "\n",
    "import model.lstmconv2d_encoder_decoder as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global config parameters\n",
    "INPUT_SEQ_LENGTH = 10\n",
    "OUTPUT_SEQ_LENGTH = 10\n",
    "\n",
    "FRAME_WIDTH = 64\n",
    "FRAME_HEIGHT = 64\n",
    "FRAME_CHANNELS = 1\n",
    "\n",
    "MOVING_AVERAGE_DECAY = 0.9999\n",
    "NUM_EPOCHS_PER_DECAY = 75.0\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.2\n",
    "INITIAL_LEARNING_RATE = 0.001\n",
    "\n",
    "LAMBDA = 5e-4\n",
    "\n",
    "BATCH_SIZE = 16  # per GPU!\n",
    "\n",
    "TRAIN_DIR = 'train_moving_mnist'\n",
    "MAX_STEPS = 30000\n",
    "\n",
    "GPU_MEMORY_FRACTION = 1.0\n",
    "GPU_ALLOW_GROWTH = True\n",
    "NUM_GPUS = 2\n",
    "MEMORY_DEVICE = '/cpu:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_train = tt.datasets.moving_mnist.MovingMNISTTrainDataset(BATCH_SIZE * NUM_GPUS,\n",
    "                                                                INPUT_SEQ_LENGTH + OUTPUT_SEQ_LENGTH)\n",
    "dataset_valid = tt.datasets.moving_mnist.MovingMNISTValidDataset(BATCH_SIZE * NUM_GPUS,\n",
    "                                                                INPUT_SEQ_LENGTH + OUTPUT_SEQ_LENGTH)\n",
    "# dataset_test = tt.datasets.moving_mnist.MovingMNISTTestDataset(BATCH_SIZE * NUM_GPUS,\n",
    "#                                                                INPUT_SEQ_LENGTH + OUTPUT_SEQ_LENGTH)\n",
    "\n",
    "# For manual verification of used parameters\n",
    "print(\"Learning rate decay every {} steps\".format(NUM_EPOCHS_PER_DECAY * dataset_train.dataset_size // (BATCH_SIZE*NUM_GPUS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tower_loss(x, y_):\n",
    "    \"\"\"Calculate the total loss on a single tower.\n",
    "    Args:\n",
    "        scope: unique prefix string identifying the tower, e.g. 'tower_0'\n",
    "    Returns:\n",
    "        Tensor of shape [] containing the total loss for a batch of data\n",
    "    \"\"\"\n",
    "    # Build inference Graph.\n",
    "    predictions = model.inference(x, \n",
    "                                  FRAME_CHANNELS, \n",
    "                                  INPUT_SEQ_LENGTH, \n",
    "                                  LAMBDA)\n",
    "\n",
    "    # Build the portion of the Graph calculating the losses. Note that we will\n",
    "    # assemble the total_loss using a custom function below.\n",
    "    total_loss, loss = model.loss(predictions, y_)\n",
    "\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    # Generate moving averages of all losses and associated summaries\n",
    "    with tf.device('/cpu:0'):\n",
    "        loss_averages_op = tt.board.loss_summary([total_loss, loss] +\n",
    "                                                 tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        total_loss = tf.identity(total_loss)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(global_step):\n",
    "    \"\"\"Train sequence model.\n",
    "    Create an optimizer and apply to all trainable variables. Add moving\n",
    "    average for all trainable variables.\n",
    "    Args:\n",
    "        total_cost: Total loss from loss function including regularization terms.\n",
    "        cost: Raw loss from loss function without regularization terms.\n",
    "        global_step: Integer Variable counting the number of training steps\n",
    "                     processed.\n",
    "    Returns:\n",
    "        train_op: op for training.\n",
    "    \"\"\"\n",
    "    # Variables that affect learning rate\n",
    "    num_batches_per_epoch = dataset_train.dataset_size / (BATCH_SIZE * NUM_GPUS)\n",
    "    decay_steps = num_batches_per_epoch * NUM_EPOCHS_PER_DECAY\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps\n",
    "    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                    global_step,\n",
    "                                    decay_steps,\n",
    "                                    LEARNING_RATE_DECAY_FACTOR,\n",
    "                                    staircase=True)\n",
    "    tf.scalar_summary('learning_rate', lr)\n",
    "\n",
    "    # Compute gradients\n",
    "    opt = tf.train.AdamOptimizer(lr)\n",
    "    # Calculate the gradients for each model tower.\n",
    "    tower_grads = []\n",
    "    tower_losses = []\n",
    "    for i in xrange(NUM_GPUS):\n",
    "        with tf.device('/gpu:%d' % i, ):\n",
    "            with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
    "                # Calculate the loss for one tower of the model. This function constructs \n",
    "                # the entire model but shares the variables across all towers.\n",
    "                t_loss = tower_loss(x[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :, :, :, :],\n",
    "                                    y_[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :, :, :, :])\n",
    "                tower_losses.append(t_loss)\n",
    "\n",
    "                # Reuse variables for the next tower.\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                # Calculate the gradients for the batch of data on this tower.\n",
    "                grads = opt.compute_gradients(t_loss)\n",
    "\n",
    "                # Keep track of the gradients ackeep_probross all towers.\n",
    "                tower_grads.append(grads)\n",
    "\n",
    "    # We must calculate the mean of each gradient.\n",
    "    # This is also the synchronization point across all towers.\n",
    "    grads = tt.training.average_gradients(tower_grads)\n",
    "    \n",
    "    total_loss = tf.reduce_mean(tower_losses)\n",
    "\n",
    "    # Apply the gradients to adjust the shared variables.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    tt.board.variables_histogram_summary()\n",
    "\n",
    "    # Add histograms for gradients\n",
    "    tt.board.gradients_histogram_summary(grads)\n",
    "\n",
    "    # Track the moving averages of all trainable variables\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # Group all updates to into a single train op.\n",
    "    train_op = tf.group(apply_gradient_op, variables_averages_op, name=\"train_op\")\n",
    "\n",
    "    return train_op, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Session (Main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_SEQ_LENGTH, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS], \"X\")\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_SEQ_LENGTH, FRAME_HEIGHT, FRAME_WIDTH, FRAME_CHANNELS], \"Y_\")\n",
    "    \n",
    "    # train the model\n",
    "    train_op, total_loss = train(global_step)\n",
    "\n",
    "    # Create a saver and merge all summaries\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    \n",
    "    # Create the graph, etc.\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    \n",
    "    # Create a session for running operations in the Graph\n",
    "    gpu_options = tf.GPUOptions(\n",
    "        per_process_gpu_memory_fraction=GPU_MEMORY_FRACTION,\n",
    "        allow_growth=GPU_ALLOW_GROWTH)\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        \n",
    "        # Initialize the variables (like the epoch counter)\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Visualize graph\n",
    "        tt.visualization.show_graph(sess.graph_def)\n",
    "        \n",
    "        # Start input enqueue threads\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        summary_writer = tf.train.SummaryWriter(TRAIN_DIR, sess.graph)\n",
    "        \n",
    "        dataset_train.reset()\n",
    "\n",
    "        try:\n",
    "            step = 0\n",
    "            while not coord.should_stop():\n",
    "                step += 1\n",
    "                \n",
    "                if (step > MAX_STEPS):\n",
    "                    break\n",
    "\n",
    "                start_time = time.time()\n",
    "                \n",
    "                batch = dataset_train.get_batch()      \n",
    "                batch_x = batch[:,0:INPUT_SEQ_LENGTH,:,:,:]\n",
    "                batch_y = batch[:,INPUT_SEQ_LENGTH:INPUT_SEQ_LENGTH+OUTPUT_SEQ_LENGTH,:,:,:]\n",
    "\n",
    "                _, loss_value = sess.run([train_op, total_loss],\n",
    "                                         feed_dict={x: batch_x, y_: batch_y})\n",
    "                duration = time.time() - start_time\n",
    "\n",
    "                assert not np.isnan(loss_value), 'Model diverged with cost = NaN'\n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    num_examples_per_step = BATCH_SIZE * NUM_GPUS\n",
    "                    examples_per_sec = num_examples_per_step / duration\n",
    "                    sec_per_batch = float(duration)\n",
    "\n",
    "                    format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                                  'sec/batch)')\n",
    "                    print (format_str % (datetime.now().time(), step, loss_value,\n",
    "                                         examples_per_sec, sec_per_batch))\n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    summary_str = sess.run(summary_op, feed_dict={x: batch_x, y_: batch_y})\n",
    "                    summary_writer.add_summary(summary_str, step)\n",
    "                    summary_writer.flush() \n",
    "\n",
    "                # Save the model checkpoint periodically.\n",
    "                if step % 1000 == 0 or (step + 1) == MAX_STEPS:\n",
    "                    checkpoint_path = os.path.join(TRAIN_DIR, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # When done, ask the threads to stop\n",
    "            coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
